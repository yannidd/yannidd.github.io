<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-08-29T20:16:19+01:00</updated><id>/feed.xml</id><title type="html">yannidd‚Äôs website</title><subtitle>Hi! I'm Yanni and this is my website. Here you can find some of my past projects, and hopefuly (not too far into the future) some blogs.</subtitle><entry><title type="html">Classification And Regression</title><link href="/2018/12/08/classification-and-regression.html" rel="alternate" type="text/html" title="Classification And Regression" /><published>2018-12-08T00:00:00+00:00</published><updated>2018-12-08T00:00:00+00:00</updated><id>/2018/12/08/classification-and-regression</id><content type="html" xml:base="/2018/12/08/classification-and-regression.html">&lt;!-----------------------------------------------------------------------------&gt;

&lt;!-----------------------------------------------------------------------------&gt;

&lt;p&gt;This is my report for a coursework from the Foundations of Machine Learning course I took at the University of Southampton. The tasks were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implementing and analysing Fisher‚Äôs LDA for binary classification by finding the best projection vector (by looking at all possible vectors).&lt;/li&gt;
  &lt;li&gt;Finding the optimal vector by solving the generalised eigenvalue condition for optimal weights and using it to perform LDA on the Iris dataset.&lt;/li&gt;
  &lt;li&gt;Implementing and performing linear regression with non-linear functions (analytically and with gradient descent).&lt;/li&gt;
  &lt;li&gt;Analysing how gradient descent generalises (looking at the bias-variance tradeof).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All algorithms were implemented from scratch in &lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; was used for the plots. I apologise for any unclarities, I had to cram everything in six A4 pages with the tightest margins.&lt;/p&gt;

&lt;h1&gt;1. Separating Two Gaussians&lt;/h1&gt;
&lt;p&gt;The covariance matrices were calculated using (1) [1]:&lt;/p&gt;
&lt;div class=&quot;eqn&quot;&gt;
  $$\
  \Sigma = 
  \begin{pmatrix} 
    S_x^2 &amp;amp; \rho S_X S_Y \\ 
    \rho S_X S_Y &amp;amp; S_Y^2 
  \end{pmatrix} 
  $$
  &lt;div class=&quot;eqncaption&quot;&gt;
    (1)
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1 shows the histograms of the two classes projected on three different direction vectors. The expectation is that a vector $\omega$ which is parallel to the centroids of the two distributions (Figure 1 - left) would result in a good separation of the projected histograms. On the other hand, $\omega$ that is perpendicular to the centroids (Figure 1 - right) results in an overlap. The subplot in the middle shows a vector which is in between the mentioned extremes. The results match the expectations. The middle plot has a clearer separation than the right one, but a worse separation than the one to the left.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image001.png&quot; /&gt;
  &lt;figcaption&gt;
    Figure 1. The probability densities of $x_a$ and $x_b$ projected on three different direction vectors $\omega$. In addition to the histograms, there is a KDE plot (solid line) and the actual gaussian distribution derived from the generating parameters (dashed line). The black dashed lines show the optimal Bayes decision boundary, whilst the solid ones show the estimated boundary by the LDA. It is worth noting, that when the distributions are overlapping, and the standard deviation of one is much higher than the other (right plot), there are two decision boundaries. This can also be noticed in the 2D log-odds.  
  &lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;This separation can be scored by the Fisher ratio, which takes the square of the difference of the projected means and scales it in accordance to the standard deviations and the relative sizes of the classes. To do this, any vector $\omega$ (in this case $\omega=(0,1)$) can be picked and rotated by a range of angles between $0 \; rad$ and $\pi \; rad$ (see Figure 2). Since we only care about the direction of the vector, the function will be periodic with a period of $\pi \; rad$. The ideal direction vector from the Fisher ratio is reasonably close to the one predicted in Figure 1.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image003.png&quot; /&gt;
  &lt;figcaption&gt;
    Figure 2. Fischer ratio for a range of $\theta$ angles. The peak value is $13.48$ and it corresponds to $\omega^‚àó = (‚àí0.96,0.27)$. This is the $(0,1)$ vector rotated by $1.30 \; rad$.
  &lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Figure 3 shows the optimal ùùéùùé vector together with equiprobable contour lines of the two distributions. The clear separation of the projections (Figure 3 - right) further confirms that this is the optimal vector. The overlap in the projection exists because the original 2D data is also slightly overlapped.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image005.png&quot; /&gt;
  &lt;figcaption&gt;
    Figure 3. Equiprobable lines for the two classes and the optimal ùùéùùé vector (left) and histograms of the projections (right). It is worth noting that ùùéùùé is not exactly parallel to the centroids of the distributions. This is because of the difference between the covariance matrices and the relative sizes of the distributions (this affects the denominator of the Fisher ratio).
  &lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;div class=&quot;eqn&quot;&gt;
  $$\
  ln\left( \frac{P(c=a|x^n)}{P(c=b|x^n)} \right)
  = ln\left( \frac{P(x^n|c=a)}{P(x^n|c=b)} \frac{P(c=a)}{P(c=b)} \right)
  = ln\left( \frac{P(x^n|c=a)}{P(x^n|c=b)} \right) + ln\left( \frac{P(c=a)}{P(c=b)} \right)
  $$
  &lt;div class=&quot;eqncaption&quot;&gt;
    (2)
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The log-odds ( 2 ) of the distributions can be rearranged using Bayes‚Äô theorem. This reveals that the log-odds are the sum of the logarithms of the ratios between the prior and posterior probabilities. The prior probability is ùëÉùëÉ(ùëêùëê)=ùëõùëõùëêùëêùëõùëõ, where n is the sum of elements in all classes [2].&lt;/p&gt;

&lt;div class=&quot;eqn&quot;&gt;
  $$\
  P(x^n|c)=
  \frac{\sqrt{|\mathbf{\Sigma_c}^{-1}|}}{2\pi} \
  exp \left( - \frac{1}{2} \mathbf{(x^n - m_c)^T \Sigma_c^{-1} (x^n - m_c)} \right )
  $$
  &lt;div class=&quot;eqncaption&quot;&gt;
    (3)
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;2-iris-dataset&quot;&gt;2. Iris Dataset&lt;/h2&gt;
&lt;h2 id=&quot;3-linear-regression-with-non-linear-functions&quot;&gt;3. Linear Regression with non-Linear Functions&lt;/h2&gt;
&lt;h3 id=&quot;31-performing-linear-regression&quot;&gt;3.1. Performing Linear Regression&lt;/h3&gt;
&lt;h3 id=&quot;32-how-does-linear-regression-generalise&quot;&gt;3.2. How Does Linear Regression Generalise&lt;/h3&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;script src=&quot;/assets/js/katex_render.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image011.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image015.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image017.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image019.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image021.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image023.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image024.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image025.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image028.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image030.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image032.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image037.png&quot; /&gt;
&lt;img src=&quot;/assets/posts/classification-and-regression/report_files/image039.png&quot; /&gt;&lt;/p&gt;</content><author><name>yannidd</name></author><summary type="html"></summary></entry></feed>