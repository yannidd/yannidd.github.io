---
title: Artificial Neural Network in the Analogue Domain
author: <a href="/">Yanislav Donchev</a>
abstract: The computation of real-time artificial neural networks can be a bottleneck for certain applications. In this article I present my work on building an artificial neural network that operates in the analogue domain.
---

<meta charset="utf-8">

<!-- JS Includes -->
<!----------------------------------------------------------------------------->
<!-- bibtex_js -->
<script 
  type="text/javascript" 
  src="https://cdn.jsdelivr.net/npm/bibtex-parser-js@0.0.2/bibtexParse.min.js"></script>
<!-- mathjax -->
<script src="js/mathjax_config.js" defer></script>
<script 
  src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
<script 
  type="text/javascript" 
  id="MathJax-script" 
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer></script>

<!-- Main Content -->
<!----------------------------------------------------------------------------->
<div class='main-content'>
  <h2>Introduction</h2>
  <p>Machine learning is used all around these days - from tasks as basic as price estimation to self-driving cars, walking robots and medical diagnosis. Different algorithms are used for each task, but what is common between them is the fact that they are all represented digitally. To compute them, a processor does linear algebra one clock cycle at a time. As you can imagine, this can easily become a bottleneck for large neural networks that need to operate in real-time. In this article I present my work on developing a neural network, which operates fully in the analogue domain. The network is built only using basic electronic components such as resistors, transistors and op-amps. The inputs, outputs and all other values within the network are electronic voltage signals.</p>
  
  <p>After reading this article, you will be able to realise an all electronic neural network, which has the exact same mathematical representation as a normal neural network. The network supports tanh and ReLU activations. I will also provide an example use case with a network that learns the XOR problem. To avoid writing an extremely lengthy article, I have skipped any derivations. However, I have referenced some excellent books by very clever people if you want more details. In my explanations I assume that you know the basics of feedforward neural networks and analogue electronic circuits.</p>
  
  <h2>The Feedforward Neural Network</h2>
  <p>Arguably the most basic neural network - and probably the first one everyone learns about - is called the feedforward neural network. In it, data flows from the inputs, through all inside neurons and then to the outputs. This section will briefly explain, with the aid of mathematical equations, how this type of network generates an output from a set of inputs; this is commonly known as forward propagation.</p>
  
  <h3>A Single Artificial Neuron</h3>
  <p>To be able to understand the network, we should first look at its building block - the artificial neuron (see Figure <a href='#fg-neuron'></a>). An artificial neuron does three things. First, it multiplies all inputs, including a special input called the bias, with their corresponding weights. It then sums all the weighted inputs. Last, it passes the summation through a non-linear function called the activation of a neuron.</p>

  <figure id="fg-neuron">
    <img src='img/single_artificial_neuron.jpg'>
    <figcaption>A graphical representation of an artificial neuron. Note that for simplicity, the bias inputs are usually not drawn.</figcaption>
  </figure>

  <p>The weighing and summing can be seen as a single step and is expressed as follows:</p>

  <p>
    $$
    \label{eqn-weighsum}
    \tag{}
    f(\mathbf{x}) = \sum_{i=0}^{N}{x_i \omega_i} + b 
    $$
  </p>

  <p>where $x_i$ are the inputs, $w_i$ are their corresponding weights, $N$ is the number of inputs and $b$ is the bias. This expression is convenient, since, as it will become apparent soon, describes a well known electronic circuit.</p>

  <p>Activation functions are non-linear functions that either squish an infinite space into a finite one (tanh and sigmoid) or rectify it (ReLU) (see Figure <a href='#fg-activations'></a>). They are used in every neural network cell, which allows the network to classify data that is not linearly separable.</p>
  
  <figure id="fg-activations">
    <div class='ext-html' src='img/activations.html'></div>
    <figcaption>An example of the most common activation functions and their mathematical representations. (Equations reproduced from <a href='#rf-ML_REFINED'></a>)</figcaption>
  </figure>

  <p>If the activation function is denoted as $g(\cdot)$, then the full expression of a neuron becomes:</p>
  
  <p>
    $$
    \label{eqn-neuron}
    \tag{}
    y = g \left ( \sum_{i=0}^{N}{x_i \omega_i} + b \right ) 
    $$
  </p>
  
  <p>There are other activation functions, but these are the most common ones. In this article I will concentrate on the tanh and ReLU activations.</p>

  <h3>A Network of Artificial Neurons</h3>
  <p>We can now take these artificial neurons and connect them into a network as in the figure below. </p>

  <figure id="fg-sample_network">
    <img class="" src='img/sample_network.jpg'>
    <figcaption>A sample network of artificial neurons. Typically neurons are stacked in layers. They are named the input layer (left), the output layer (right) and the hidden layers (all layers in the middle).</figcaption>
  </figure>

  <h2>Modelling a Neuron with Electronic Circuits</h2>
  <p>In this section I will introduce the electronic circuits required to build an artificial neuron.</p>
  
  <h3>Weighing and Summing</h3>
  <p>Modelling the weighing and sumation half of the neuron using op-amps is relatively straightforward. If you have had experience with analogue electronics before, you have most probably come accross a circuit called a voltage adder (Figure <a href="#fg-v_adder"></a>). As the name suggests, this circuit outputs the sum of some voltages. Even better, it weighs these input voltages depending on the values of the input resistors. $V_O$ is the weighted sum of the inputs $V_i$.</p>

  <figure id='fg-v_adder'>
    <img src="img/voltage_adder.png">
    <figcaption>
      A voltage adder. The output $V_O$ is the weighted sum of the inputs $V_i$. The weight of each input $V_i$ is controlled by its corresponding input resistor value $R_i$. (Adapted from <a href="#rf-V_ADDER"></a>).
    </figcaption>
  </figure>

  <p>The input to output expression of this circuit is:</p>
  
  <p>
    $$
    \label{eqn-v_adder}
    \tag{}
    V_o = - R_f \sum_{i=0}^{N}{\frac{V_i}{R_i}} 
    $$
  </p>

  <p>This looks very similar to the summing part of a neuron. The input voltages $V_i$ correspond to the neuron inputs $x_i$, the resistance ratios $R_f / R_i$ correspond to the weights $\omega_i$ and the output voltage $V_o$ corresponds to the neuron output $y$.</p>

  <p>If you have been paying attention, you might have noticed a few problems with this model. First of all, there is a minus sign in the adder expression (the output is inverted in electronic engineering terms). Next, if any of the weights $w_i$ is negative then the corresponding resistor value $R_i$ should also be negative. Lastly, the sum of the inputs in the neuron is unbounded, but the sum in the op-amp is bounded by the supply rail voltages.</p>

  <p>The first problem is trivial to solve. A voltage inverter after the voltage adder will remove the minus sign.</p>

  <figure id='fg-v_inverter'>
    <img src="img/v_inverter.png">
    <figcaption>
      A voltage inverter. If the value of both resistors is the same, the output voltage is equal to the negative of the input voltage.
    </figcaption>
  </figure>
  
  <p>For the second problem, we can use normal resistors and whenever a negative weight occurs, we can add a voltage inverter to that input. This will work because inverting the weight or inverting the input to the neuron has the same overall effect.</p> 

  <p>Unfortunately, there is no solution to the last problem. The best thing to do is to hope that your neural network will never have a sum that is higher than the supply rails (typically $\pm15V$). Luckily this happens rarely. For reassurance, you could plot a histogram of the internal neural network values.</p>

  <h3>The ReLU Activation</h3>
  <p>Another common circuit can be used to model the ReLU activation. This circuit is called the precision rectifier and is shown in the figure below. For any negative input voltage, this circuits outputs 0. For any positive input voltage, this circuit outputs its inverted value. Therefore, once again we would need to add an inverter to its output.</p>

  <h3>The tanh Activation</h3>
  <p>The tanh activation will be somewhat more involved.</p>

  <a href="#rf-V_ADDER"></a>
  <a href="#fn-example"></a>
</div>

<!-- Footnotes -->
<!----------------------------------------------------------------------------->
<h5>Footnotes</h5>
<div class='footnotes'>
  <ol>
    <li id='fn-example'>Example</li>
  </ol>
</div>

<!-- References -->
<!----------------------------------------------------------------------------->
<h5>References</h5>
<div class='references'></div>

<!-- Scripts -->
<!----------------------------------------------------------------------------->
<script src="js/footnotes.js"></script>
<script src="js/references.js"></script>
<script src="js/numberize.js"></script>
<script src="js/load_external.js" defer></script>